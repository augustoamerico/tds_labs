{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider each intervention as a document, and therefore the corpus is the union of all interventions in a given session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords as StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_TO_SPLIT_DOCUMENTS = \"(O|A)+\\s+Sr(\\.|\\.º|\\.ª)\\s+([A-zÀ-ú]|\\s*)+(\\(.*\\))?: —\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O Sr. Presidente: —'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Sr.ª Alexandra Viera (BE)\n",
      "A Sr.ª Bebiana Cunha (PAN)\n",
      "A Sr.ª Carla Madureira (PSD)\n",
      "A Sr.ª Catarina Rocha Ferreira (PSD)\n",
      "A Sr.ª Clarisse Campos (PS)\n",
      "A Sr.ª Cristina Mendes da Silva (PS)\n",
      "A Sr.ª Diana Ferreira (PCP)\n",
      "A Sr.ª Emília Cerqueira (PSD)\n",
      "A Sr.ª Fabíola Cardoso (BE)\n",
      "A Sr.ª Inês de Sousa Real (PAN)\n",
      "A Sr.ª Isabel Pires (BE)\n",
      "A Sr.ª Lina Lopes (PSD)\n",
      "A Sr.ª Mariana Silva (PEV)\n",
      "A Sr.ª Sandra Cunha (BE)\n",
      "A Sr.ª Secretária (Maria da Luz Rosinha)\n",
      "A Sr.ª Sofia Matos (PSD)\n",
      "O Sr. André Ventura (CH)\n",
      "O Sr. Bruno Dias (PCP)\n",
      "O Sr. Cristóvão Norte (PSD)\n",
      "O Sr. Duarte Alves (PCP)\n",
      "O Sr. Hugo Carvalho (PS)\n",
      "O Sr. Jerónimo de Sousa (PCP)\n",
      "O Sr. Jorge Costa (BE)\n",
      "O Sr. Jorge Salgueiro Mendes (PSD)\n",
      "O Sr. José Luís Ferreira (PEV)\n",
      "O Sr. José Moura Soeiro (BE)\n",
      "O Sr. João Cotrim de Figueiredo (IL)\n",
      "O Sr. João Dias (PCP)\n",
      "O Sr. João Gonçalves Pereira (CDS-PP)\n",
      "O Sr. João Oliveira (PCP)\n",
      "O Sr. João Pinho de Almeida (CDS-PP)\n",
      "O Sr. Nelson Basílio Silva (PAN)\n",
      "O Sr. Nuno Fazenda (PS)\n",
      "O Sr. Pedro do Carmo (PS)\n",
      "O Sr. Presidente\n",
      "O Sr. Presidente (José Manuel Pureza)\n",
      "O Sr. Ricardo Vicente (BE)\n",
      "O Sr. Tiago Barbosa Ribeiro (PS)\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# Get the entities\n",
    "\n",
    "deputies_and_president = set()\n",
    "\n",
    "with open(\"resources/example_of_parlamentar_discussion/darl14sl02n014.txt\") as file:\n",
    "    pattern = re.compile(REGEX_TO_SPLIT_DOCUMENTS)\n",
    "    for line in file:\n",
    "        match = pattern.search(line)\n",
    "        if match is not None:\n",
    "            deputies_and_president.add(match.group()[0:-3])\n",
    "\n",
    "for el in sorted(deputies_and_president):\n",
    "    print(el)\n",
    "print(len(deputies_and_president))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate documents to entity\n",
    "\n",
    "deputies_docs_unprocessed = {}\n",
    "documents_unprocessed_idx = {}\n",
    "documents_to_deputies = {}\n",
    "\n",
    "doc_idx = 0\n",
    "did_first_match = False\n",
    "\n",
    "with open(\"resources/example_of_parlamentar_discussion/darl14sl02n014.txt\") as file:\n",
    "    \n",
    "    first_line = next(file)\n",
    "    DATE_SECTION_REGEX = \"(?i)\\d+ de (\\w+) de \\d{4}\"\n",
    "    romanic_number = \"(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})\"\n",
    "    SERIES_SECTION_REGEX = romanic_number + \" (Série|SÉRIE) — (Número|NÚMERO) \\d{1,3}\"\n",
    "    \n",
    "    \n",
    "    pattern = re.compile(REGEX_TO_SPLIT_DOCUMENTS)\n",
    "    date_section_pattern = re.compile(DATE_SECTION_REGEX)\n",
    "    series_section_pattern = re.compile(SERIES_SECTION_REGEX)\n",
    "    numberic_pattern = re.compile(\"\\d+\")\n",
    "    \n",
    "    current_docs = \"\"\n",
    "    current_deputy = None\n",
    "    \n",
    "    for line in file:\n",
    "        date_section_match = date_section_pattern.search(line)\n",
    "        series_section_match = series_section_pattern.search(line)\n",
    "        if date_section_match is not None or series_section_match is not None:\n",
    "            #we are in a section, let's consume until a number appear\n",
    "            line_is_page_number = False\n",
    "            while not line_is_page_number:\n",
    "                #check if line is number\n",
    "                #if it is, then line_is_page_number = True\n",
    "                line = next(file)\n",
    "                numeric_match = numberic_pattern.search(line)\n",
    "                if numeric_match is not None:\n",
    "                    line_is_page_number = True\n",
    "                    line = next(file)\n",
    "        match = pattern.search(line)\n",
    "        if match is not None:\n",
    "            #a new document\n",
    "            #is this the first one? if it is, then we already consumed the summary section\n",
    "            if current_deputy is not None:\n",
    "                #save current document\n",
    "                documents_unprocessed_idx[doc_idx] = current_docs\n",
    "                if current_deputy not in deputies_docs_unprocessed:\n",
    "                    deputies_docs_unprocessed[current_deputy] = []\n",
    "                deputies_docs_unprocessed[current_deputy].append(doc_idx)\n",
    "                documents_to_deputies[doc_idx] = current_deputy\n",
    "                doc_idx += 1\n",
    "            #docs stored. start processing new one\n",
    "            current_deputy = match.group()[0:-3]\n",
    "            current_docs = line.replace(current_deputy, '')\n",
    "        else:\n",
    "            current_docs += line\n",
    "    if current_deputy is not None:\n",
    "                #save current document\n",
    "                documents_unprocessed_idx[doc_idx] = current_docs\n",
    "                if current_deputy not in deputies_docs_unprocessed:\n",
    "                    deputies_docs_unprocessed[current_deputy] = []\n",
    "                deputies_docs_unprocessed[current_deputy].append(doc_idx)\n",
    "                documents_to_deputies[doc_idx] = current_deputy\n",
    "                doc_idx += 1\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize docs\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "ps = RSLPStemmer()\n",
    "pt_stop_words = StopWords.words('portuguese')\n",
    "\n",
    "def mytokeniser(s):\n",
    "    aux = filter(lambda x: x not in pt_stop_words , [w.lower() for w in tokenizer.tokenize(s)])\n",
    "    return list(map(ps.stem, aux))\n",
    "\n",
    "documents_tokenized = {}\n",
    "\n",
    "for idx in documents_unprocessed_idx:\n",
    "    documents_tokenized[idx] = mytokeniser(documents_unprocessed_idx[idx])\n",
    "\n",
    "tokenized_corpus = list(documents_tokenized.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_be_removed = []\n",
    "for idx in range(0, len(tokenized_corpus)):\n",
    "    if len(tokenized_corpus[idx]) == 0:\n",
    "        deputies_docs_unprocessed[documents_to_deputies[idx]].remove(idx)\n",
    "        docs_to_be_removed.append(idx)\n",
    "        \n",
    "for el in sorted(docs_to_be_removed, reverse=True):\n",
    "    del tokenized_corpus[el]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My vocabolary size is 2460\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus_idf_search = list(map(set, tokenized_corpus))\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    vocab = vocab.union(set(doc))  \n",
    "\n",
    "print(f\"My vocabolary size is {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idfvocab = {}\n",
    "\n",
    "def idf(term, corpus):\n",
    "    cnt =  sum([1 if term in doc else 0 for doc in corpus])\n",
    "    return math.log10( len(corpus) / cnt )\n",
    "\n",
    "for term in vocab:\n",
    "    term_idf = idf(term, tokenized_corpus_idf_search)\n",
    "    idfvocab[term] = term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min is 0.1595031217250559 and max is 2.3873898263387296\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "idfvocab_it = [(el[0],el[1]) for el in idfvocab.items()]\n",
    "\n",
    "aux = np.array( idfvocab_it )\n",
    "low = float( min( aux[:,1] ) )\n",
    "high = float( max( aux[:,1] ) )\n",
    "\n",
    "print(f\"Min is {low} and max is {high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_terms( lower, upper, threshold, step, idf_vocabulary ):\n",
    "    low = lower\n",
    "    up = upper\n",
    "    candidates = idf_vocabulary\n",
    "    while len(candidates) > threshold:\n",
    "        #print(f\"current vocabolary size is {len(candidates)}\")\n",
    "        low = low + step\n",
    "        up = up - step\n",
    "        candidates = [  term for term in idf_vocabulary if term[1] >= low and term[1] <= up  ]\n",
    "    return candidates\n",
    "\n",
    "\n",
    "cnd = keep_terms(low, high, int(len(idfvocab_it)*0.2), 0.005, idfvocab_it)\n",
    "len(cnd)\n",
    "\n",
    "vc = np.array(cnd) #a matrix, with column 0 being terms and column 1 being idf\n",
    "vc_terms = vc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normTFx(term,doc):\n",
    "    return doc.count(term)/len(doc)\n",
    "\n",
    "def tfidfmat(corpus, tl, idfvocab) :\n",
    "    mat =[]\n",
    "    for term in tl :\n",
    "        idft = idfvocab[term]\n",
    "        row = []\n",
    "        for doc in corpus:\n",
    "            tft = normTFx(term,doc)\n",
    "            tf_idf_term_document = tft*idft\n",
    "            row.append(tf_idf_term_document)\n",
    "        mat.append(row)\n",
    "    return mat    \n",
    "            \n",
    "    \n",
    "\n",
    "tfidf_matrix = tfidfmat(tokenized_corpus, vc_terms, idfvocab) \n",
    "tfidf_matrix_np = np.array(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an optimized implementation for te creation of the TF.IDF Matrix, that takes less than half the time than the implementation above\n",
    "\n",
    "def normTFx_optimized(term, docMapCount, docLength):\n",
    "    '''\n",
    "        If you compare this implementation with the above one, you can notice that:\n",
    "           - docMapCount is a dictionary where the key is the term, and the value is the count of the term in that document\n",
    "           - the length of the document is received as a parameter\n",
    "        This leads that, in this implementation, for each time we need to count a term in a document, we get that in constant time\n",
    "    '''\n",
    "    return (docMapCount.get(term) or 0)/docLength\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def reduce_doc_map_count(reduced, el):\n",
    "    if el in reduced:\n",
    "        reduced[el] += 1\n",
    "    else:\n",
    "        reduced[el] = 1\n",
    "    return reduced\n",
    "\n",
    "\n",
    "'''\n",
    "    the bellow code is doing the same as the following:\n",
    "    for doc in tokenized_corpus_sampled:\n",
    "        docMapCount = {}\n",
    "        for term in doc:\n",
    "            if term in docMapCount:\n",
    "                docMapCount[term] += 1\n",
    "            else:\n",
    "                docMapCount[term] = 1\n",
    "'''\n",
    "tokenized_corpus_map_count = list(map( lambda doc: reduce( reduce_doc_map_count , doc, {}  ) , tokenized_corpus  ))\n",
    "\n",
    "def tfidfmat(corpusMapCount, corpus, tl,idfvocab) :\n",
    "    mat =[]\n",
    "    for term in tl :\n",
    "        idft = idfvocab[term]\n",
    "        row = []\n",
    "        for i in range(len(corpus)):\n",
    "            tft = normTFx_optimized(term,corpusMapCount[i],len(corpus[i]))\n",
    "            tf_idf_term_document = tft*idft\n",
    "            row.append(tf_idf_term_document)\n",
    "        mat.append(row)\n",
    "    return mat    \n",
    "            \n",
    "    \n",
    "\n",
    "tfidf_matrix = tfidfmat(tokenized_corpus_map_count, tokenized_corpus, vc_terms, idfvocab) \n",
    "tfidf_matrix_np = np.array(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488, 244)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document with index 0 contains 50 words\n",
      "The term with index 0 is `2021`\n",
      "The importance of the term `2021` in the document with idx = 0 is 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The document with index 0 contains {len(tokenized_corpus[0])} words\")\n",
    "print(f\"The term with index 0 is `{vc_terms[0]}`\")\n",
    "\n",
    "print(f\"The importance of the term `{vc_terms[0]}` in the document with idx = 0 is {tfidf_matrix_np[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=4, init='random', random_state=0)\n",
    "W = model.fit_transform(tfidf_matrix_np) # loadings\n",
    "H = model.components_ #scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_N_terms(matrix_slice, N):\n",
    "    return matrix_slice.argsort()[-N:]\n",
    "\n",
    "def get_terms_from_slice(loadings_matrix, idx, topN, bag_of_words, orientation=\"col\"):\n",
    "    '''\n",
    "        the parameter `orientation` can either be \"col\" or \"row\", so we can process a loadings matrix being it transposed or not\n",
    "    '''\n",
    "    k = None\n",
    "    if orientation == \"col\":\n",
    "        k = loadings_matrix[:,idx]\n",
    "    elif orientation == \"row\":\n",
    "        k = loadings_matrix[idx,:]\n",
    "    else:\n",
    "        raise Exception(\"Orientation not recognized\")\n",
    "    k_top5terms_idx = get_top_N_terms(k,topN)\n",
    "    return bag_of_words[k_top5terms_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The terms with more weight in the component 0 are: ['mal' 'senh' 'ness' 'assin' 'fic' 'ouv' 'prim' 'ide' 'ministr' 'verdad']\n",
      "The terms with more weight in the component 1 are: ['falt' 'próx' 'vou' 'ferr' 'faç' 'favor' 'peç' 'centr' 'que' 'conclu']\n",
      "The terms with more weight in the component 2 are: ['pens' 'gost' 'falt' 'vou' 'próx' 'segund' 'aind' 'fal' 'nad' 'diss']\n",
      "The terms with more weight in the component 3 are: ['mesm' '2' 'utiliz' 'inform' 'desperdíci' 'temp' 'segund' 'encerr'\n",
      " 'proced' 'poi']\n"
     ]
    }
   ],
   "source": [
    "top_words = 10\n",
    "for k in range(0,W.shape[1]):\n",
    "    # Get terms for the k-th characteristic / topic\n",
    "    print(f\"The terms with more weight in the component {k} are: {get_terms_from_slice(W, k, top_words, vc_terms)}\")\n",
    "\n",
    "# here we are printing the top 7, but the this choise is arbitrary - we are going to analyze as much as we need to understand the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6.458058222330159e-06),\n",
       " (0, 4.6677365711131104e-05),\n",
       " (2, 0.00011070709499117414),\n",
       " (1, 0.001041829682280001)]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(enumerate(H[:,deputies_docs_unprocessed[\"O Sr. André Ventura (CH)\"][4]])), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 244)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
