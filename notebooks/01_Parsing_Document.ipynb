{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Parsing Parliament Discussion Document\"\n",
    "output: html_document\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "reticulate::use_condaenv(condaenv=\"/home/tds/anaconda3\", required = T)\n",
    "PROJHOME <- rprojroot::find_rstudio_root_file()\n",
    "reticulate::py_run_string(paste0(\"PROJHOME='\",PROJHOME,\"'\"))\n",
    "#reticulate::r_to_py(PROJHOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider each intervention as a document, and therefore the corpus is the union of all interventions in a given session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying extracting Intervinients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_TO_SPLIT_DOCUMENTS = \"(O|A)+\\s+Sr(\\.|\\.º|\\.ª)\\s+([A-zÀ-ú]|\\s*)+(\\(.*\\))?: —\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get the entities\n",
    "\n",
    "deputies_and_president = set()\n",
    "\n",
    "with open(os.path.join(PROJHOME,\"resources/example_of_parlamentar_discussion/darl14sl02n014.txt\")) as file:\n",
    "    pattern = re.compile(REGEX_TO_SPLIT_DOCUMENTS)\n",
    "    for line in file:\n",
    "        match = pattern.search(line)\n",
    "        if match is not None:\n",
    "            deputies_and_president.add(match.group()[0:-3])\n",
    "\n",
    "for el in sorted(deputies_and_president):\n",
    "    print(el)\n",
    "print(len(deputies_and_president))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to create the list of (Intervinient, Intervention) and its mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Associate documents to entity\n",
    "\n",
    "deputies_docs_unprocessed = {}\n",
    "documents_unprocessed_idx = {}\n",
    "documents_to_deputies = {}\n",
    "\n",
    "doc_idx = 0\n",
    "did_first_match = False\n",
    "\n",
    "with open(\"resources/example_of_parlamentar_discussion/darl14sl02n014.txt\") as file:\n",
    "    \n",
    "    first_line = next(file)\n",
    "    DATE_SECTION_REGEX = \"(?i)\\d+ de (\\w+) de \\d{4}\"\n",
    "    romanic_number = \"(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})\"\n",
    "    SERIES_SECTION_REGEX = romanic_number + \" (Série|SÉRIE) — (Número|NÚMERO) \\d{1,3}\"\n",
    "    \n",
    "    \n",
    "    pattern = re.compile(REGEX_TO_SPLIT_DOCUMENTS)\n",
    "    date_section_pattern = re.compile(DATE_SECTION_REGEX)\n",
    "    series_section_pattern = re.compile(SERIES_SECTION_REGEX)\n",
    "    numberic_pattern = re.compile(\"\\d+\")\n",
    "    \n",
    "    current_docs = \"\"\n",
    "    current_deputy = None\n",
    "    \n",
    "    for line in file:\n",
    "        date_section_match = date_section_pattern.search(line)\n",
    "        series_section_match = series_section_pattern.search(line)\n",
    "        if date_section_match is not None or series_section_match is not None:\n",
    "            #we are in a section, let's consume until a number appear\n",
    "            line_is_page_number = False\n",
    "            while not line_is_page_number:\n",
    "                #check if line is number\n",
    "                #if it is, then line_is_page_number = True\n",
    "                line = next(file)\n",
    "                numeric_match = numberic_pattern.search(line)\n",
    "                if numeric_match is not None:\n",
    "                    line_is_page_number = True\n",
    "                    line = next(file)\n",
    "        match = pattern.search(line)\n",
    "        if match is not None:\n",
    "            #a new document\n",
    "            #is this the first one? if it is, then we already consumed the summary section\n",
    "            if current_deputy is not None:\n",
    "                #save current document\n",
    "                documents_unprocessed_idx[doc_idx] = current_docs\n",
    "                if current_deputy not in deputies_docs_unprocessed:\n",
    "                    deputies_docs_unprocessed[current_deputy] = []\n",
    "                deputies_docs_unprocessed[current_deputy].append(doc_idx)\n",
    "                documents_to_deputies[doc_idx] = current_deputy\n",
    "                doc_idx += 1\n",
    "            #docs stored. start processing new one\n",
    "            current_deputy = match.group()[0:-3]\n",
    "            current_docs = line.replace(current_deputy, '')\n",
    "        else:\n",
    "            current_docs += line\n",
    "    if current_deputy is not None:\n",
    "                #save current document\n",
    "                documents_unprocessed_idx[doc_idx] = current_docs\n",
    "                if current_deputy not in deputies_docs_unprocessed:\n",
    "                    deputies_docs_unprocessed[current_deputy] = []\n",
    "                deputies_docs_unprocessed[current_deputy].append(doc_idx)\n",
    "                documents_to_deputies[doc_idx] = current_deputy\n",
    "                doc_idx += 1\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,name,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
