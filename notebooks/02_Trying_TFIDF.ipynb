{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TF-IDF and NMF on Parlamentar Activity\"\n",
    "output: html_document\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "reticulate::use_condaenv(condaenv=\"/home/tds/anaconda3\", required = T)\n",
    "PROJHOME <- rprojroot::find_rstudio_root_file()\n",
    "reticulate::py_run_string(paste0(\"PROJHOME='\",PROJHOME,\"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.join(PROJHOME,\"src\",\"modules\"))\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords as StopWords\n",
    "from parliament_discussions_document_parser import ParliamentDiscussionsDocumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "parliament_discussion_document_path = os.path.join(\n",
    "    PROJHOME,\"resources/example_of_parlamentar_discussion/darl14sl02n014.txt\"\n",
    "    )\n",
    "deputies_docs_unprocessed, documents_unprocessed_idx, documents_to_deputies = ParliamentDiscussionsDocumentParser(\n",
    "    parliament_discussion_document_path).parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "ps = RSLPStemmer()\n",
    "pt_stop_words = StopWords.words('portuguese')\n",
    "\n",
    "def mytokeniser(s):\n",
    "    aux = filter(lambda x: x not in pt_stop_words , [w.lower() for w in tokenizer.tokenize(s)])\n",
    "    return list(map(ps.stem, aux))\n",
    "\n",
    "documents_tokenized = {}\n",
    "\n",
    "for idx in documents_unprocessed_idx:\n",
    "    documents_tokenized[idx] = mytokeniser(documents_unprocessed_idx[idx])\n",
    "\n",
    "tokenized_corpus = list(documents_tokenized.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "docs_to_be_removed = []\n",
    "for idx in range(0, len(tokenized_corpus)):\n",
    "    if len(tokenized_corpus[idx]) == 0:\n",
    "        deputies_docs_unprocessed[documents_to_deputies[idx]].remove(idx)\n",
    "        docs_to_be_removed.append(idx)\n",
    "        \n",
    "for el in sorted(docs_to_be_removed, reverse=True):\n",
    "    del tokenized_corpus[el]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_corpus_idf_search = list(map(set, tokenized_corpus))\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    vocab = vocab.union(set(doc))  \n",
    "\n",
    "print(f\"My vocabolary size is {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idfvocab = {}\n",
    "\n",
    "def idf(term, corpus):\n",
    "    cnt =  sum([1 if term in doc else 0 for doc in corpus])\n",
    "    return math.log10( len(corpus) / cnt )\n",
    "\n",
    "for term in vocab:\n",
    "    term_idf = idf(term, tokenized_corpus_idf_search)\n",
    "    idfvocab[term] = term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idfvocab_it = [(el[0],el[1]) for el in idfvocab.items()]\n",
    "\n",
    "aux = np.array( idfvocab_it )\n",
    "low = float( min( aux[:,1] ) )\n",
    "high = float( max( aux[:,1] ) )\n",
    "\n",
    "print(f\"Min is {low} and max is {high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def keep_terms( lower, upper, threshold, step, idf_vocabulary ):\n",
    "    low = lower\n",
    "    up = upper\n",
    "    candidates = idf_vocabulary\n",
    "    while len(candidates) > threshold:\n",
    "        #print(f\"current vocabolary size is {len(candidates)}\")\n",
    "        low = low + step\n",
    "        up = up - step\n",
    "        candidates = [  term for term in idf_vocabulary if term[1] >= low and term[1] <= up  ]\n",
    "    return candidates\n",
    "\n",
    "\n",
    "#cnd = keep_terms(low, high, int(len(idfvocab_it)*0.2), 0.005, idfvocab_it)\n",
    "cnd = keep_terms(low, high, len(idfvocab_it), 0.005, idfvocab_it)\n",
    "len(cnd)\n",
    "\n",
    "vc = np.array(cnd) #a matrix, with column 0 being terms and column 1 being idf\n",
    "vc_terms = vc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an optimized implementation for te creation of the TF.IDF Matrix, that takes less than half the time than the implementation above\n",
    "\n",
    "def normTFx_optimized(term, docMapCount, docLength):\n",
    "    return (docMapCount.get(term) or 0)/docLength\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def reduce_doc_map_count(reduced, el):\n",
    "    if el in reduced:\n",
    "        reduced[el] += 1\n",
    "    else:\n",
    "        reduced[el] = 1\n",
    "    return reduced\n",
    "\n",
    "tokenized_corpus_map_count = list(map( lambda doc: reduce( reduce_doc_map_count , doc, {}  ) , tokenized_corpus  ))\n",
    "\n",
    "def tfidfmat(corpusMapCount, corpus, tl,idfvocab) :\n",
    "    mat =[]\n",
    "    for term in tl :\n",
    "        idft = idfvocab[term]\n",
    "        row = []\n",
    "        for i in range(len(corpus)):\n",
    "            tft = normTFx_optimized(term,corpusMapCount[i],len(corpus[i]))\n",
    "            tf_idf_term_document = tft*idft\n",
    "            row.append(tf_idf_term_document)\n",
    "        mat.append(row)\n",
    "    return mat    \n",
    "            \n",
    "\n",
    "tfidf_matrix = tfidfmat(tokenized_corpus_map_count, tokenized_corpus, vc_terms, idfvocab) \n",
    "tfidf_matrix_np = np.array(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The document with index 0 contains {len(tokenized_corpus[0])} words\")\n",
    "print(f\"The term with index 0 is `{vc_terms[0]}`\")\n",
    "\n",
    "print(f\"The importance of the term `{vc_terms[0]}` in the document with idx = 0 is {tfidf_matrix_np[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=4, init='random', random_state=1)\n",
    "W = model.fit_transform(tfidf_matrix_np) # loadings\n",
    "H = model.components_ #scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_top_N_terms(matrix_slice, N):\n",
    "    return matrix_slice.argsort()[-N:]\n",
    "\n",
    "def get_terms_from_slice(loadings_matrix, idx, topN, bag_of_words, orientation=\"col\"):\n",
    "    '''\n",
    "        the parameter `orientation` can either be \"col\" or \"row\", so we can process a loadings matrix being it transposed or not\n",
    "    '''\n",
    "    k = None\n",
    "    if orientation == \"col\":\n",
    "        k = loadings_matrix[:,idx]\n",
    "    elif orientation == \"row\":\n",
    "        k = loadings_matrix[idx,:]\n",
    "    else:\n",
    "        raise Exception(\"Orientation not recognized\")\n",
    "    k_top5terms_idx = get_top_N_terms(k,topN)\n",
    "    return bag_of_words[k_top5terms_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "top_words = 10\n",
    "for k in range(0,W.shape[1]):\n",
    "    # Get terms for the k-th characteristic / topic\n",
    "    print(f\"The terms with more weight in the component {k} are: {get_terms_from_slice(W, k, top_words, vc_terms)}\")\n",
    "\n",
    "# here we are printing the top 7, but the this choise is arbitrary - we are going to analyze as much as we need to understand the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "name,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
