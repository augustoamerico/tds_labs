{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TF-IDF of extracted entities from interventions on Parlamentar Activity\"\n",
    "output: html_document\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::opts_chunk$set(echo = TRUE)\n",
    "reticulate::use_condaenv(condaenv=\"/home/tds/anaconda3\", required = T)\n",
    "PROJHOME <- rprojroot::find_rstudio_root_file()\n",
    "reticulate::py_run_string(paste0(\"PROJHOME='\",PROJHOME,\"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "from typing import List, Tuple\n",
    "from nltk.corpus import stopwords as StopWords\n",
    "\n",
    "sys.path.insert(0, os.path.join(PROJHOME,\"src\",\"modules\"))\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "parliament_discussion_document_path = os.path.join(\n",
    "    PROJHOME,\"resources/example_of_parlamentar_discussion/darl14sl02n014.txt\"\n",
    "    )\n",
    "deputies_docs_unprocessed, documents_unprocessed_idx, documents_to_deputies = ParliamentDiscussionsDocumentParser(\n",
    "    parliament_discussion_document_path).parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pt_stop_words = StopWords.words('portuguese')\n",
    "\n",
    "def parse_entities(document: str) -> List[str]:\n",
    "    nlp_doc = nlp(document)\n",
    "    return [re.sub(' +', ' ', ent.text.replace(\"\\n\",\" \")) for ent in nlp_doc.ents]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_processed_entities = {}\n",
    "\n",
    "for idx in documents_unprocessed_idx:\n",
    "    documents_processed_entities[idx] = parse_entities(documents_unprocessed_idx[idx])\n",
    "\n",
    "entities_corpus = list(documents_processed_entities.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "docs_to_be_removed = []\n",
    "for idx in documents_processed_entities.keys():\n",
    "    if len(documents_processed_entities[idx]) == 0:\n",
    "        docs_to_be_removed.append(idx)\n",
    "        deputies_docs_unprocessed[documents_to_deputies[idx]].remove(idx)\n",
    "        del documents_to_deputies[idx]\n",
    "\n",
    "for el in sorted(docs_to_be_removed, reverse=True):\n",
    "    del documents_processed_entities[el]\n",
    "    \n",
    "entities_corpus = list(documents_processed_entities.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_corpus_idf_search = list(map(set, entities_corpus))\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for doc in entities_corpus:\n",
    "    vocab = vocab.union(set(doc))  \n",
    "\n",
    "print(f\"My vocabolary size is {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idfvocab = {}\n",
    "\n",
    "def idf(term, corpus):\n",
    "    cnt =  sum([1 if term in doc else 0 for doc in corpus])\n",
    "    return math.log10( len(corpus) / cnt )\n",
    "\n",
    "for term in vocab:\n",
    "    term_idf = idf(term, entities_corpus_idf_search)\n",
    "    idfvocab[term] = term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idfvocab_it = [(el[0],el[1]) for el in idfvocab.items()]\n",
    "\n",
    "aux = np.array( idfvocab_it )\n",
    "low = float( min( aux[:,1] ) )\n",
    "high = float( max( aux[:,1] ) )\n",
    "\n",
    "print(f\"Min is {low} and max is {high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = np.array(idfvocab_it) #a matrix, with column 0 being terms and column 1 being idf\n",
    "vc_terms = vc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normTFx(term,doc):\n",
    "    return doc.count(term)/len(doc)\n",
    "\n",
    "def tfidfmat(corpus, tl, idfvocab) :\n",
    "    mat =[]\n",
    "    for term in tl :\n",
    "        idft = idfvocab[term]\n",
    "        row = []\n",
    "        for doc in corpus:\n",
    "            tft = normTFx(term,doc)\n",
    "            tf_idf_term_document = tft*idft\n",
    "            row.append(tf_idf_term_document)\n",
    "        mat.append(row)\n",
    "    return mat    \n",
    "            \n",
    "    \n",
    "\n",
    "tfidf_matrix = tfidfmat(entities_corpus, vc_terms, idfvocab) \n",
    "tfidf_matrix_np = np.array(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "assert len(list(documents_processed_entities.keys())) == len(documents_to_deputies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_entities(idx_entities_scored: List[Tuple[int, float]]):\n",
    "    for idx, score in idx_entities_scored:\n",
    "        print(f\"{vc_terms[idx]} - {round(score,3)}\")\n",
    "\n",
    "def column_in_tfidf_of_document_with_index(document_index):\n",
    "    return list(documents_processed_entities.keys()).index(document_index)\n",
    "        \n",
    "def print_info_for_document(document_idx):\n",
    "    print(documents_to_deputies[document_idx])\n",
    "    column_idx = column_in_tfidf_of_document_with_index(document_idx)\n",
    "    print_entities(list(filter(lambda x: x[1] > 0, sorted(enumerate(tfidf_matrix_np[:,column_idx].tolist()), key=lambda x: x[1], reverse=True))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print_info_for_document(deputies_docs_unprocessed[\"O Sr. André Ventura (CH)\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "documents_unprocessed_idx[deputies_docs_unprocessed[\"O Sr. André Ventura (CH)\"][12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "name,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
