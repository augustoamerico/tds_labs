[["index.html", "Extracting keywords from Parliament Discussions Interventions 1 Parsing Parliament Discussion’s Documents 1.1 Trying extracting Intervinients 1.2 Let’s try to create the list of (Intervinient, Intervention) and its mappings 1.3 Keyword Extraction", " Extracting keywords from Parliament Discussions Interventions Tiago dos Santos 2020-11-04 1 Parsing Parliament Discussion’s Documents Let’s consider each intervention as a document, and therefore the corpus is the union of all interventions in a given session import re import os 1.1 Trying extracting Intervinients REGEX_TO_SPLIT_DOCUMENTS = &quot;(O|A)+\\s+Sr(\\.|\\.º|\\.ª)\\s+([A-zÀ-ú]|\\s*)+(\\(.*\\))?: —&quot; # Get the entities deputies_and_president = set() file_path = os.path.join(PROJHOME,&quot;..&quot;,&quot;resources/example_of_parlamentar_discussion/darl14sl02n014.txt&quot;) with open(file_path) as file: pattern = re.compile(REGEX_TO_SPLIT_DOCUMENTS) for line in file: match = pattern.search(line) if match is not None: deputies_and_president.add(match.group()[0:-3]) for el in sorted(deputies_and_president): print(el) ## A Sr.ª Alexandra Viera (BE) ## A Sr.ª Bebiana Cunha (PAN) ## A Sr.ª Carla Madureira (PSD) ## A Sr.ª Catarina Rocha Ferreira (PSD) ## A Sr.ª Clarisse Campos (PS) ## A Sr.ª Cristina Mendes da Silva (PS) ## A Sr.ª Diana Ferreira (PCP) ## A Sr.ª Emília Cerqueira (PSD) ## A Sr.ª Fabíola Cardoso (BE) ## A Sr.ª Inês de Sousa Real (PAN) ## A Sr.ª Isabel Pires (BE) ## A Sr.ª Lina Lopes (PSD) ## A Sr.ª Mariana Silva (PEV) ## A Sr.ª Sandra Cunha (BE) ## A Sr.ª Secretária (Maria da Luz Rosinha) ## A Sr.ª Sofia Matos (PSD) ## O Sr. André Ventura (CH) ## O Sr. Bruno Dias (PCP) ## O Sr. Cristóvão Norte (PSD) ## O Sr. Duarte Alves (PCP) ## O Sr. Hugo Carvalho (PS) ## O Sr. Jerónimo de Sousa (PCP) ## O Sr. Jorge Costa (BE) ## O Sr. Jorge Salgueiro Mendes (PSD) ## O Sr. José Luís Ferreira (PEV) ## O Sr. José Moura Soeiro (BE) ## O Sr. João Cotrim de Figueiredo (IL) ## O Sr. João Dias (PCP) ## O Sr. João Gonçalves Pereira (CDS-PP) ## O Sr. João Oliveira (PCP) ## O Sr. João Pinho de Almeida (CDS-PP) ## O Sr. Nelson Basílio Silva (PAN) ## O Sr. Nuno Fazenda (PS) ## O Sr. Pedro do Carmo (PS) ## O Sr. Presidente ## O Sr. Presidente (José Manuel Pureza) ## O Sr. Ricardo Vicente (BE) ## O Sr. Tiago Barbosa Ribeiro (PS) print(len(deputies_and_president)) ## 38 1.2 Let’s try to create the list of (Intervinient, Intervention) and its mappings # Associate documents to entity deputies_docs_unprocessed = {} documents_unprocessed_idx = {} documents_to_deputies = {} doc_idx = 0 did_first_match = False with open(file_path) as file: first_line = next(file) DATE_SECTION_REGEX = &quot;(?i)\\d+ de (\\w+) de \\d{4}&quot; romanic_number = &quot;(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})&quot; SERIES_SECTION_REGEX = romanic_number + &quot; (Série|SÉRIE) — (Número|NÚMERO) \\d{1,3}&quot; pattern = re.compile(REGEX_TO_SPLIT_DOCUMENTS) date_section_pattern = re.compile(DATE_SECTION_REGEX) series_section_pattern = re.compile(SERIES_SECTION_REGEX) numberic_pattern = re.compile(&quot;\\d+&quot;) current_docs = &quot;&quot; current_deputy = None for line in file: date_section_match = date_section_pattern.search(line) series_section_match = series_section_pattern.search(line) if date_section_match is not None or series_section_match is not None: #we are in a section, let&#39;s consume until a number appear line_is_page_number = False while not line_is_page_number: #check if line is number #if it is, then line_is_page_number = True line = next(file) numeric_match = numberic_pattern.search(line) if numeric_match is not None: line_is_page_number = True line = next(file) match = pattern.search(line) if match is not None: #a new document #is this the first one? if it is, then we already consumed the summary section if current_deputy is not None: #save current document documents_unprocessed_idx[doc_idx] = current_docs if current_deputy not in deputies_docs_unprocessed: deputies_docs_unprocessed[current_deputy] = [] deputies_docs_unprocessed[current_deputy].append(doc_idx) documents_to_deputies[doc_idx] = current_deputy doc_idx += 1 #docs stored. start processing new one current_deputy = match.group()[0:-3] current_docs = line.replace(current_deputy, &#39;&#39;) else: current_docs += line if current_deputy is not None: #save current document documents_unprocessed_idx[doc_idx] = current_docs if current_deputy not in deputies_docs_unprocessed: deputies_docs_unprocessed[current_deputy] = [] deputies_docs_unprocessed[current_deputy].append(doc_idx) documents_to_deputies[doc_idx] = current_deputy doc_idx += 1 1.3 Keyword Extraction In this notebook, we are going to point out on trying two python packages for keyword extraction: - (YAKE)[https://github.com/LIAAD/yake] - (pke)[https://github.com/boudinfl/pke] Let’s start with YAKE 1.3.1 import yake from typing import List, Tuple, Dict sys.path.insert(0, os.path.join(PROJHOME,&quot;modules&quot;)) from parliament_discussions_document_parser import ParliamentDiscussionsDocumentParser parliament_discussion_document_path = os.path.join( PROJHOME,&quot;..&quot;,&quot;resources/example_of_parlamentar_discussion/darl14sl02n014.txt&quot; ) deputies_docs_unprocessed, documents_unprocessed_idx, documents_to_deputies = ParliamentDiscussionsDocumentParser( parliament_discussion_document_path).parse() def concatenate_all_interventions(interventions_idx: List[int], interventions_idx_map: Dict[int, str]): concatenated_interventions = &quot;&quot; for idx in interventions_idx: concatenated_interventions += interventions_idx_map[idx] + &quot;\\n&quot; return concatenated_interventions print(deputies_docs_unprocessed.keys()) ## dict_keys([&#39;O Sr. Presidente&#39;, &#39;A Sr.ª Secretária (Maria da Luz Rosinha)&#39;, &#39;A Sr.ª Inês de Sousa Real (PAN)&#39;, &#39;O Sr. João Dias (PCP)&#39;, &#39;O Sr. Jerónimo de Sousa (PCP)&#39;, &#39;A Sr.ª Mariana Silva (PEV)&#39;, &#39;A Sr.ª Catarina Rocha Ferreira (PSD)&#39;, &#39;O Sr. João Gonçalves Pereira (CDS-PP)&#39;, &#39;O Sr. Ricardo Vicente (BE)&#39;, &#39;A Sr.ª Alexandra Viera (BE)&#39;, &#39;A Sr.ª Clarisse Campos (PS)&#39;, &#39;O Sr. Pedro do Carmo (PS)&#39;, &#39;O Sr. André Ventura (CH)&#39;, &#39;O Sr. João Oliveira (PCP)&#39;, &#39;A Sr.ª Emília Cerqueira (PSD)&#39;, &#39;O Sr. José Luís Ferreira (PEV)&#39;, &#39;O Sr. Bruno Dias (PCP)&#39;, &#39;O Sr. Duarte Alves (PCP)&#39;, &#39;A Sr.ª Isabel Pires (BE)&#39;, &#39;O Sr. João Cotrim de Figueiredo (IL)&#39;, &#39;A Sr.ª Sofia Matos (PSD)&#39;, &#39;O Sr. Hugo Carvalho (PS)&#39;, &#39;O Sr. Cristóvão Norte (PSD)&#39;, &#39;O Sr. João Pinho de Almeida (CDS-PP)&#39;, &#39;A Sr.ª Sandra Cunha (BE)&#39;, &#39;O Sr. Jorge Salgueiro Mendes (PSD)&#39;, &#39;O Sr. Nuno Fazenda (PS)&#39;, &#39;O Sr. Nelson Basílio Silva (PAN)&#39;, &#39;O Sr. Presidente (José Manuel Pureza)&#39;, &#39;A Sr.ª Fabíola Cardoso (BE)&#39;, &#39;A Sr.ª Carla Madureira (PSD)&#39;, &#39;A Sr.ª Cristina Mendes da Silva (PS)&#39;, &#39;A Sr.ª Diana Ferreira (PCP)&#39;, &#39;A Sr.ª Bebiana Cunha (PAN)&#39;, &#39;O Sr. José Moura Soeiro (BE)&#39;, &#39;O Sr. Jorge Costa (BE)&#39;, &#39;A Sr.ª Lina Lopes (PSD)&#39;, &#39;O Sr. Tiago Barbosa Ribeiro (PS)&#39;]) text = concatenate_all_interventions(deputies_docs_unprocessed[&#39;O Sr. Jerónimo de Sousa (PCP)&#39;], documents_unprocessed_idx) print(text) ## : — Muito bem! def generate_keywords_from(intervinient: str, lan: str = &quot;pt&quot;, ngram: int = 5, deduplicate_threshold = 0.5): text = concatenate_all_interventions(deputies_docs_unprocessed[intervinient], documents_unprocessed_idx) kw_extractor = yake.KeywordExtractor(lan=lan,n=ngram,dedupLim=deduplicate_threshold) return dict(kw_extractor.extract_keywords(text)) #df_keywords = reticulate::py_capture_output(&quot;generate_keywords_from(&#39;O Sr. André Ventura (CH)&#39;)&quot;) df_keywords = as.data.frame(t(as.data.frame(reticulate::py$generate_keywords_from(&#39;O Sr. André Ventura (CH)&#39;)))) df_keywords$keywords = c(rownames(df_keywords)) colnames(df_keywords) = c(&quot;scores&quot;,&quot;keywords&quot;) df_keywords$scores = df_keywords$scores/min(df_keywords$scores) - min(df_keywords$scores/min(df_keywords$scores))*1000 colnames(df_keywords) = c(&quot;freq&quot;,&quot;word&quot;) df_keywords %&gt;% echarts4r::e_color_range(freq, color) %&gt;% echarts4r::e_charts() %&gt;% echarts4r::e_cloud(word, freq, color, shape = &quot;circle&quot;, sizeRange = c(3, 15)) %&gt;% echarts4r::e_title(&quot;Wordcloud&quot;, &quot;Random strings&quot;) d &lt;- data.frame(word = rownames(df_keywords), freq = df_keywords$freq) wordcloud2::wordcloud2(d) wordcloud::wordcloud(df_keywords$word, df_keywords$freq) ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## deputado.joão.cotrim.de.figueiredo could not be fit on page. It will not be ## plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## partido.socialista could not be fit on page. It will not be plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## pcp.e.de.deputados could not be fit on page. It will not be plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## esquerda.e.o.pcp could not be fit on page. It will not be plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## mesmo.governo.socialista could not be fit on page. It will not be plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## bloco.de.esquerda could not be fit on page. It will not be plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): presidente ## could not be fit on page. It will not be plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## socialista.para.fazer.o.mesmo could not be fit on page. It will not be plotted. ## Warning in wordcloud::wordcloud(df_keywords$word, df_keywords$freq): ## joão.azevedo.castro could not be fit on page. It will not be plotted. generate_wordcloud_from_intervinient &lt;- function(intervinient, ngram=3){ df_keywords = as.data.frame(t(as.data.frame(reticulate::py$generate_keywords_from(intervinient, ngram=as.integer(ngram))))) df_keywords$keywords = c(rownames(df_keywords)) colnames(df_keywords) = c(&quot;scores&quot;,&quot;keywords&quot;) df_keywords$scores = df_keywords$scores*-1 df_keywords$scores = sqrt(df_keywords$scores - min(df_keywords$scores)) colnames(df_keywords) = c(&quot;freq&quot;,&quot;word&quot;) print(df_keywords) d &lt;- data.frame(word = rownames(df_keywords), freq = df_keywords$freq) return(wordcloud2::wordcloud2(d,size = .3)) } "]]
