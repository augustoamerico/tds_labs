{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.corpus import stopwords as StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_TO_SPLIT_DOCUMENTS = \"(O|A)+\\s+Sr(\\.|\\.º|\\.ª)\\s+([A-zÀ-ú]|\\s*)+(\\(.*\\))?: —\"\n",
    "\n",
    "\n",
    "deputies_docs_unprocessed = {}\n",
    "documents_unprocessed_idx = {}\n",
    "documents_to_deputies = {}\n",
    "\n",
    "doc_idx = 0\n",
    "did_first_match = False\n",
    "\n",
    "with open(\"resources/example_of_parlamentar_discussion/darl14sl02n014.txt\") as file:\n",
    "    \n",
    "    first_line = next(file)\n",
    "    DATE_SECTION_REGEX = \"(?i)\\d+ de (\\w+) de \\d{4}\"\n",
    "    romanic_number = \"(?=[MDCLXVI])M*(C[MD]|D?C{0,3})(X[CL]|L?X{0,3})(I[XV]|V?I{0,3})\"\n",
    "    SERIES_SECTION_REGEX = romanic_number + \" (Série|SÉRIE) — (Número|NÚMERO) \\d{1,3}\"\n",
    "    \n",
    "    \n",
    "    pattern = re.compile(REGEX_TO_SPLIT_DOCUMENTS)\n",
    "    date_section_pattern = re.compile(DATE_SECTION_REGEX)\n",
    "    series_section_pattern = re.compile(SERIES_SECTION_REGEX)\n",
    "    numberic_pattern = re.compile(\"\\d+\")\n",
    "    \n",
    "    current_docs = \"\"\n",
    "    current_deputy = None\n",
    "    \n",
    "    for line in file:\n",
    "        date_section_match = date_section_pattern.search(line)\n",
    "        series_section_match = series_section_pattern.search(line)\n",
    "        if date_section_match is not None or series_section_match is not None:\n",
    "            #we are in a section, let's consume until a number appear\n",
    "            line_is_page_number = False\n",
    "            while not line_is_page_number:\n",
    "                #check if line is number\n",
    "                #if it is, then line_is_page_number = True\n",
    "                line = next(file)\n",
    "                numeric_match = numberic_pattern.search(line)\n",
    "                if numeric_match is not None:\n",
    "                    line_is_page_number = True\n",
    "                    line = next(file)\n",
    "        match = pattern.search(line)\n",
    "        if match is not None:\n",
    "            #a new document\n",
    "            #is this the first one? if it is, then we already consumed the summary section\n",
    "            if current_deputy is not None:\n",
    "                #save current document\n",
    "                documents_unprocessed_idx[doc_idx] = current_docs\n",
    "                if current_deputy not in deputies_docs_unprocessed:\n",
    "                    deputies_docs_unprocessed[current_deputy] = []\n",
    "                deputies_docs_unprocessed[current_deputy].append(doc_idx)\n",
    "                documents_to_deputies[doc_idx] = current_deputy\n",
    "                doc_idx += 1\n",
    "            #docs stored. start processing new one\n",
    "            current_deputy = match.group()[0:-3]\n",
    "            current_docs = line.replace(current_deputy, '')\n",
    "        else:\n",
    "            current_docs += line\n",
    "    if current_deputy is not None:\n",
    "                #save current document\n",
    "                documents_unprocessed_idx[doc_idx] = current_docs\n",
    "                if current_deputy not in deputies_docs_unprocessed:\n",
    "                    deputies_docs_unprocessed[current_deputy] = []\n",
    "                deputies_docs_unprocessed[current_deputy].append(doc_idx)\n",
    "                documents_to_deputies[doc_idx] = current_deputy\n",
    "                doc_idx += 1\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize docs\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "ps = RSLPStemmer()\n",
    "pt_stop_words = StopWords.words('portuguese')\n",
    "\n",
    "def mytokeniser(s):\n",
    "    aux = filter(lambda x: x not in pt_stop_words , [w.lower() for w in tokenizer.tokenize(s)])\n",
    "    return list(map(ps.stem, aux))\n",
    "\n",
    "documents_tokenized = {}\n",
    "\n",
    "for idx in documents_unprocessed_idx:\n",
    "    documents_tokenized[idx] = mytokeniser(documents_unprocessed_idx[idx])\n",
    "\n",
    "tokenized_corpus = list(documents_tokenized.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_be_removed = []\n",
    "for idx in range(0, len(tokenized_corpus)):\n",
    "    if len(tokenized_corpus[idx]) == 0:\n",
    "        deputies_docs_unprocessed[documents_to_deputies[idx]].remove(idx)\n",
    "        docs_to_be_removed.append(idx)\n",
    "        \n",
    "for el in sorted(docs_to_be_removed, reverse=True):\n",
    "    del tokenized_corpus[el]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My vocabolary size is 2460\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus_idf_search = list(map(set, tokenized_corpus))\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    vocab = vocab.union(set(doc))  \n",
    "\n",
    "print(f\"My vocabolary size is {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idfvocab = {}\n",
    "\n",
    "def idf(term, corpus):\n",
    "    cnt =  sum([1 if term in doc else 0 for doc in corpus])\n",
    "    return math.log10( len(corpus) / cnt )\n",
    "\n",
    "for term in vocab:\n",
    "    term_idf = idf(term, tokenized_corpus_idf_search)\n",
    "    idfvocab[term] = term_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min is 0.1595031217250559 and max is 2.3873898263387296\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "idfvocab_it = [(el[0],el[1]) for el in idfvocab.items()]\n",
    "\n",
    "aux = np.array( idfvocab_it )\n",
    "low = float( min( aux[:,1] ) )\n",
    "high = float( max( aux[:,1] ) )\n",
    "\n",
    "print(f\"Min is {low} and max is {high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_terms( lower, upper, threshold, step, idf_vocabulary ):\n",
    "    low = lower\n",
    "    up = upper\n",
    "    candidates = idf_vocabulary\n",
    "    while len(candidates) > threshold:\n",
    "        #print(f\"current vocabolary size is {len(candidates)}\")\n",
    "        low = low + step\n",
    "        up = up - step\n",
    "        candidates = [  term for term in idf_vocabulary if term[1] >= low and term[1] <= up  ]\n",
    "    return candidates\n",
    "\n",
    "\n",
    "#cnd = keep_terms(low, high, int(len(idfvocab_it)*0.2), 0.005, idfvocab_it)\n",
    "cnd = keep_terms(low, high, len(idfvocab_it), 0.005, idfvocab_it)\n",
    "len(cnd)\n",
    "\n",
    "vc = np.array(cnd) #a matrix, with column 0 being terms and column 1 being idf\n",
    "vc_terms = vc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normTFx(term,doc):\n",
    "    return doc.count(term)/len(doc)\n",
    "\n",
    "def tfidfmat(corpus, tl, idfvocab) :\n",
    "    mat =[]\n",
    "    for term in tl :\n",
    "        idft = idfvocab[term]\n",
    "        row = []\n",
    "        for doc in corpus:\n",
    "            tft = normTFx(term,doc)\n",
    "            tf_idf_term_document = tft*idft\n",
    "            row.append(tf_idf_term_document)\n",
    "        mat.append(row)\n",
    "    return mat    \n",
    "            \n",
    "    \n",
    "\n",
    "tfidf_matrix = tfidfmat(tokenized_corpus, vc_terms, idfvocab) \n",
    "tfidf_matrix_np = np.array(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an optimized implementation for te creation of the TF.IDF Matrix, that takes less than half the time than the implementation above\n",
    "\n",
    "def normTFx_optimized(term, docMapCount, docLength):\n",
    "    '''\n",
    "        If you compare this implementation with the above one, you can notice that:\n",
    "           - docMapCount is a dictionary where the key is the term, and the value is the count of the term in that document\n",
    "           - the length of the document is received as a parameter\n",
    "        This leads that, in this implementation, for each time we need to count a term in a document, we get that in constant time\n",
    "    '''\n",
    "    return (docMapCount.get(term) or 0)/docLength\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def reduce_doc_map_count(reduced, el):\n",
    "    if el in reduced:\n",
    "        reduced[el] += 1\n",
    "    else:\n",
    "        reduced[el] = 1\n",
    "    return reduced\n",
    "\n",
    "\n",
    "'''\n",
    "    the bellow code is doing the same as the following:\n",
    "    for doc in tokenized_corpus_sampled:\n",
    "        docMapCount = {}\n",
    "        for term in doc:\n",
    "            if term in docMapCount:\n",
    "                docMapCount[term] += 1\n",
    "            else:\n",
    "                docMapCount[term] = 1\n",
    "'''\n",
    "tokenized_corpus_map_count = list(map( lambda doc: reduce( reduce_doc_map_count , doc, {}  ) , tokenized_corpus  ))\n",
    "\n",
    "def tfidfmat(corpusMapCount, corpus, tl,idfvocab) :\n",
    "    mat =[]\n",
    "    for term in tl :\n",
    "        idft = idfvocab[term]\n",
    "        row = []\n",
    "        for i in range(len(corpus)):\n",
    "            tft = normTFx_optimized(term,corpusMapCount[i],len(corpus[i]))\n",
    "            tf_idf_term_document = tft*idft\n",
    "            row.append(tf_idf_term_document)\n",
    "        mat.append(row)\n",
    "    return mat    \n",
    "            \n",
    "    \n",
    "\n",
    "tfidf_matrix = tfidfmat(tokenized_corpus_map_count, tokenized_corpus, vc_terms, idfvocab) \n",
    "tfidf_matrix_np = np.array(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488, 244)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document with index 0 contains 50 words\n",
      "The term with index 0 is `patrã`\n",
      "The importance of the term `patrã` in the document with idx = 0 is 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The document with index 0 contains {len(tokenized_corpus[0])} words\")\n",
    "print(f\"The term with index 0 is `{vc_terms[0]}`\")\n",
    "\n",
    "print(f\"The importance of the term `{vc_terms[0]}` in the document with idx = 0 is {tfidf_matrix_np[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=4, init='random', random_state=0)\n",
    "W = model.fit_transform(tfidf_matrix_np) # loadings\n",
    "H = model.components_ #scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_N_terms(matrix_slice, N):\n",
    "    return matrix_slice.argsort()[-N:]\n",
    "\n",
    "def get_terms_from_slice(loadings_matrix, idx, topN, bag_of_words, orientation=\"col\"):\n",
    "    '''\n",
    "        the parameter `orientation` can either be \"col\" or \"row\", so we can process a loadings matrix being it transposed or not\n",
    "    '''\n",
    "    k = None\n",
    "    if orientation == \"col\":\n",
    "        k = loadings_matrix[:,idx]\n",
    "    elif orientation == \"row\":\n",
    "        k = loadings_matrix[idx,:]\n",
    "    else:\n",
    "        raise Exception(\"Orientation not recognized\")\n",
    "    k_top5terms_idx = get_top_N_terms(k,topN)\n",
    "    return bag_of_words[k_top5terms_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The terms with more weight in the component 0 are: ['mal' 'senh' 'ness' 'assin' 'fic' 'ouv' 'prim' 'ide' 'ministr' 'verdad']\n",
      "The terms with more weight in the component 1 are: ['falt' 'próx' 'vou' 'ferr' 'faç' 'favor' 'peç' 'que' 'conclu' 'centr']\n",
      "The terms with more weight in the component 2 are: ['pens' 'gost' 'falt' 'vou' 'próx' 'segund' 'aind' 'fal' 'nad' 'diss']\n",
      "The terms with more weight in the component 3 are: ['mesm' '2' 'utiliz' 'inform' 'desperdíci' 'temp' 'segund' 'encerr'\n",
      " 'proced' 'poi']\n"
     ]
    }
   ],
   "source": [
    "top_words = 10\n",
    "for k in range(0,W.shape[1]):\n",
    "    # Get terms for the k-th characteristic / topic\n",
    "    print(f\"The terms with more weight in the component {k} are: {get_terms_from_slice(W, k, top_words, vc_terms)}\")\n",
    "\n",
    "# here we are printing the top 7, but the this choise is arbitrary - we are going to analyze as much as we need to understand the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6.459847355601038e-06),\n",
       " (0, 4.6499527237510584e-05),\n",
       " (2, 0.00011765550185564994),\n",
       " (1, 0.0008421342184620323)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(enumerate(H[:,deputies_docs_unprocessed[\"O Sr. André Ventura (CH)\"][4]])), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
